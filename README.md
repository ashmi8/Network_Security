# Network Security Project - Complete Documentation

## ğŸ“‹ Table of Contents
1. Project Overview
2. Architecture & Workflow
3. Project Structure
4. Prerequisites & Setup
5. Installation & Configuration
6. Project Workflow
7. API Endpoints
8. Docker Setup
9. CI/CD Pipeline
10. Troubleshooting

---

## ğŸ¯ Project Overview

**Network Security** is an end-to-end machine learning application designed to detect phishing and network security threats using classification models. The project implements a complete MLOps pipeline with data ingestion, validation, transformation, model training, and deployment capabilities.

### Key Features
- **Data Pipeline**: MongoDB integration for data ingestion
- **Data Validation**: Schema validation and drift detection
- **Data Transformation**: KNN imputation for missing values
- **Model Training**: Multiple classification algorithms with hyperparameter tuning
- **MLflow Integration**: Experiment tracking and model monitoring
- **REST API**: FastAPI for model serving and predictions
- **Cloud Integration**: AWS S3 for artifact storage
- **Docker Support**: Containerized deployment
- **CI/CD**: GitHub Actions for automated workflows

---

## ğŸ—ï¸ Architecture & Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA SOURCE (MongoDB)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          1. DATA INGESTION (CSV Export & Split)             â”‚
â”‚  - Connects to MongoDB                                      â”‚
â”‚  - Exports collection as DataFrame                          â”‚
â”‚  - Splits into train/test sets (80/20)                      â”‚
â”‚  - Saves to: Artifacts/<timestamp>/data_ingestion/          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       2. DATA VALIDATION (Schema & Drift Detection)         â”‚
â”‚  - Validates number of columns against schema.yaml          â”‚
â”‚  - Detects data drift using KS-2 sample test               â”‚
â”‚  - Generates drift report                                   â”‚
â”‚  - Saves valid data to: data_validation/validated/          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    3. DATA TRANSFORMATION (Feature Engineering & Imputation)â”‚
â”‚  - Handles missing values with KNNImputer (k=3)             â”‚
â”‚  - Creates preprocessing pipeline                           â”‚
â”‚  - Transforms features using fitted preprocessor            â”‚
â”‚  - Saves transformed data as .npy files                     â”‚
â”‚  - Saves preprocessor object for inference                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   4. MODEL TRAINING (Hyperparameter Tuning & Evaluation)    â”‚
â”‚  - Trains multiple algorithms:                              â”‚
â”‚    â€¢ Random Forest                                          â”‚
â”‚    â€¢ Decision Tree                                          â”‚
â”‚    â€¢ Gradient Boosting                                      â”‚
â”‚    â€¢ Logistic Regression                                    â”‚
â”‚    â€¢ AdaBoost                                               â”‚
â”‚  - GridSearchCV for hyperparameter optimization             â”‚
â”‚  - Evaluates on train/test sets                             â”‚
â”‚  - Calculates metrics: F1, Precision, Recall                â”‚
â”‚  - Tracks experiments with MLflow                           â”‚
â”‚  - Selects best model based on test RÂ² score                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        5. MODEL DEPLOYMENT (S3 & Final Model Creation)      â”‚
â”‚  - Saves artifacts to S3 bucket                             â”‚
â”‚  - Creates NetworkModel wrapper (preprocessor + model)      â”‚
â”‚  - Saves final_model/ directory with:                       â”‚
â”‚    â€¢ model.pkl (trained classifier)                         â”‚
â”‚    â€¢ preprocessor.pkl (transformation pipeline)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              6. INFERENCE (REST API Endpoint)                â”‚
â”‚  - Load preprocessor and model from final_model/            â”‚
â”‚  - Create NetworkModel instance                             â”‚
â”‚  - Transform input features with preprocessor               â”‚
â”‚  - Generate predictions                                     â”‚
â”‚  - Return predictions to user                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
Network_Security/
â”œâ”€â”€ Network_security/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py          # MongoDB â†’ CSV conversion
â”‚   â”‚   â”œâ”€â”€ data_validation.py         # Schema & drift validation
â”‚   â”‚   â”œâ”€â”€ data_transformation.py     # Feature preprocessing
â”‚   â”‚   â””â”€â”€ model_trainer.py           # Model training & selection
â”‚   â”œâ”€â”€ entity/
â”‚   â”‚   â”œâ”€â”€ config_entity.py           # Configuration classes
â”‚   â”‚   â””â”€â”€ artifacts_entity.py        # Artifact data classes
â”‚   â”œâ”€â”€ exception/
â”‚   â”‚   â””â”€â”€ exception.py               # Custom exception handling
â”‚   â”œâ”€â”€ logging/
â”‚   â”‚   â””â”€â”€ logger.py                  # Logging configuration
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ main_utils/
â”‚   â”‚   â”‚   â””â”€â”€ utils.py               # Helper functions
â”‚   â”‚   â””â”€â”€ ml_utils/
â”‚   â”‚       â”œâ”€â”€ metric/
â”‚   â”‚       â”‚   â””â”€â”€ classification_metric.py   # Metrics calculation
â”‚   â”‚       â””â”€â”€ model/
â”‚   â”‚           â””â”€â”€ estimator.py       # NetworkModel wrapper
â”‚   â”œâ”€â”€ constants/
â”‚   â”‚   â””â”€â”€ training_pipeline/
â”‚   â”‚       â””â”€â”€ __init__.py            # Pipeline constants
â”‚   â”œâ”€â”€ cloud/
â”‚   â”‚   â””â”€â”€ s3_syncer.py               # AWS S3 integration
â”‚   â””â”€â”€ pipeline/
â”‚       â””â”€â”€ training_pipeline.py       # Main orchestration
â”œâ”€â”€ data_schema/
â”‚   â””â”€â”€ schema.yaml                    # Data validation schema
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ table.html                     # HTML template for predictions
â”œâ”€â”€ logs/                              # Training logs
â”œâ”€â”€ Artifacts/                         # Generated artifacts
â”œâ”€â”€ final_model/                       # Deployed model artifacts
â”œâ”€â”€ prediction_output/                 # Prediction results
â”‚
â”œâ”€â”€ app.py                             # FastAPI application
â”œâ”€â”€ main.py                            # Direct execution entry point
â”œâ”€â”€ push_data.py                       # MongoDB data loader
â”‚
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ setup.py                           # Package configuration
â”œâ”€â”€ DOCKERFILE                         # Container configuration
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ main.yml                   # GitHub Actions CI/CD
â”œâ”€â”€ .env                               # Environment variables
â”œâ”€â”€ .env.example                       # Environment template
â”œâ”€â”€ .gitignore                         # Git ignore rules
â”œâ”€â”€ LICENSE                            # GPL v3 License
â””â”€â”€ README.md                          # This file
```

---

## ğŸ”§ Installation & Configuration

### Step 1: Clone the Repository
```bash
git clone https://github.com/ashmijha/Network_Security.git
cd Network_Security
```

### Step 2: Create Virtual Environment
```bash
# Create virtual environment
python -m venv env

# Activate virtual environment
# On Linux/macOS:
source env/bin/activate

# On Windows:
env\Scripts\activate
```

### Step 3: Install Dependencies
```bash
pip install -r requirements.txt
```

### Step 4: Configure Environment Variables
```bash
# Create .env file from template
cp .env.example .env

# Edit .env with your credentials
nano .env
```

**Required .env variables:**
```bash
MONGODB_URI=mongodb+srv://<username>:<password>@<cluster>.mongodb.net/<database>?retryWrites=true&w=majority
AWS_ACCESS_KEY_ID=your_aws_access_key
AWS_SECRET_ACCESS_KEY=your_aws_secret_key
AWS_DEFAULT_REGION=us-east-1
```

### Step 5: Load Data into MongoDB
```bash
# Run the data push script
python push_data.py

# This will:
# 1. Read Network_Data/phisingData.csv
# 2. Convert CSV to JSON records
# 3. Insert into MongoDB ASHMI_DB.NetworkData collection
```

---

## ğŸš€ Project Workflow

### Option 1: Direct Python Execution
```bash
# Run the main training pipeline
python main.py

# Or use the modular approach in main.py:
# - Data Ingestion
# - Data Validation
# - Data Transformation
# - Model Training
```

### Option 2: FastAPI Application
```bash
# Start the API server
python app.py

# The server will run at: http://localhost:8000

# Access interactive docs:
# - Swagger UI: http://localhost:8000/docs
# - ReDoc: http://localhost:8000/redoc
```

### Option 3: Using Docker
```bash
# Build the Docker image
docker build -t network-security:latest .

# Run the container
docker run -p 8000:8000 \
  -e MONGODB_URI="your_mongodb_uri" \
  -e AWS_ACCESS_KEY_ID="your_aws_key" \
  -e AWS_SECRET_ACCESS_KEY="your_aws_secret" \
  network-security:latest
```

### Detailed Workflow Steps

#### 1ï¸âƒ£ Data Ingestion
```python
# Loads data from MongoDB
# File: Network_security/components/data_ingestion.py

DataIngestion:
  â”œâ”€â”€ export_collection_as_dataframe()
  â”‚   â””â”€â”€ Connects to MongoDB and fetches NetworkData
  â”œâ”€â”€ export_data_into_feature_store()
  â”‚   â””â”€â”€ Saves full dataset as CSV
  â””â”€â”€ split_data_as_train_test()
      â”œâ”€â”€ 80% training data
      â””â”€â”€ 20% testing data
```

**Output Artifacts:**
- `Artifacts/<timestamp>/data_ingestion/feature_store/phisingData.csv`
- `Artifacts/<timestamp>/data_ingestion/ingested/train.csv`
- `Artifacts/<timestamp>/data_ingestion/ingested/test.csv`

#### 2ï¸âƒ£ Data Validation
```python
# Validates data quality and detects drift
# File: Network_security/components/data_validation.py

DataValidation:
  â”œâ”€â”€ validate_number_of_columns()
  â”‚   â””â”€â”€ Checks against data_schema/schema.yaml
  â””â”€â”€ detect_dataset_drift()
      â””â”€â”€ Uses Kolmogorov-Smirnov test (p-value threshold: 0.05)
```

**Output Artifacts:**
- `Artifacts/<timestamp>/data_validation/validated/train.csv`
- `Artifacts/<timestamp>/data_validation/validated/test.csv`
- `Artifacts/<timestamp>/data_validation/drift_report/report.yaml`

#### 3ï¸âƒ£ Data Transformation
```python
# Transforms features using KNN imputation
# File: Network_security/components/data_transformation.py

DataTransformation:
  â”œâ”€â”€ get_data_transformer_object()
  â”‚   â””â”€â”€ Creates Pipeline with KNNImputer(n_neighbors=3)
  â””â”€â”€ initiate_data_transformation()
      â”œâ”€â”€ Fits preprocessor on training data
      â”œâ”€â”€ Transforms train features
      â”œâ”€â”€ Transforms test features
      â””â”€â”€ Appends target column
```

**Output Artifacts:**
- `Artifacts/<timestamp>/data_transformation/transformed/train.npy`
- `Artifacts/<timestamp>/data_transformation/transformed/test.npy`
- `Artifacts/<timestamp>/data_transformation/transformed_object/preprocessing.pkl`
- `final_model/preprocessor.pkl` (for inference)

#### 4ï¸âƒ£ Model Training
```python
# Trains and selects best model
# File: Network_security/components/model_trainer.py

ModelTrainer:
  â”œâ”€â”€ train_model()
  â”‚   â”œâ”€â”€ Initialize 5 classifiers
  â”‚   â”œâ”€â”€ GridSearchCV for hyperparameters
  â”‚   â”œâ”€â”€ Train and evaluate each model
  â”‚   â””â”€â”€ Select best by test score
  â”œâ”€â”€ track_mlflow()
  â”‚   â””â”€â”€ Log metrics: F1, Precision, Recall
  â””â”€â”€ initiate_model_trainer()
      â””â”€â”€ Create ModelTrainerArtifact
```

**Models Trained:**
| Model | Hyperparameters |
|-------|-----------------|
| Random Forest | n_estimators: [8, 16, 32, 128, 256] |
| Decision Tree | criterion: ['gini', 'entropy', 'log_loss'] |
| Gradient Boosting | learning_rate: [0.1, 0.01, 0.05, 0.001] |
| Logistic Regression | default |
| AdaBoost | n_estimators, learning_rate |

**Output Artifacts:**
- `Artifacts/<timestamp>/model_trainer/trained_model/model.pkl`
- `final_model/model.pkl` (for inference)

---

## ğŸ”Œ API Endpoints

### 1. Home / Docs Redirect
```
GET /
```
Redirects to Swagger documentation at `/docs`

### 2. Training Pipeline
```
GET /train
```
**Description**: Triggers the complete training pipeline

**Response**:
```json
{
  "message": "Training is successful"
}
```

**Example**:
```bash
curl -X GET "http://localhost:8000/train"
```

### 3. Prediction
```
POST /predict
```
**Description**: Uploads CSV file and returns predictions

**Parameters**:
- `file` (multipart/form-data): CSV file with features

**Response**: HTML table with predictions

**Example**:
```bash
curl -X POST "http://localhost:8000/predict" \
  -F "file=@input_data.csv"
```

**Input CSV Format**:
```csv
feature1,feature2,feature3,...,featureN
0.5,0.3,0.8,...,0.2
0.6,0.4,0.7,...,0.3
```

---

## ğŸ³ Docker Setup

### Dockerfile Explanation
```dockerfile
FROM python:3.9-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y awscli git

# Copy requirements
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy project
COPY . .

# Install package
RUN pip install -e .

# Expose port
EXPOSE 8000

# Run application
CMD ["python", "app.py"]
```

### Build and Run
```bash
# Build image
docker build -t network-security:latest .

# Run container with environment variables
docker run -d \
  --name network-security \
  -p 8000:8000 \
  -e MONGODB_URI="$MONGODB_URI" \
  -e AWS_ACCESS_KEY_ID="$AWS_ACCESS_KEY_ID" \
  -e AWS_SECRET_ACCESS_KEY="$AWS_SECRET_ACCESS_KEY" \
  network-security:latest

# View logs
docker logs -f network-security

# Stop container
docker stop network-security
```

---

## âš™ï¸ CI/CD Pipeline

### GitHub Actions Workflow
```yaml
# File: .github/workflows/main.yml
```

**Workflow Steps:**

1. **Trigger**: On push to `main` branch
2. **Checkout**: Clone repository
3. **Setup Python**: Install Python 3.9
4. **Install Dependencies**: `pip install -r requirements.txt`
5. **Linting** (optional): Code quality checks
6. **Run Tests** (optional): Unit tests
7. **Build Docker Image**: Create container
8. **Push to Registry** (optional): Docker Hub or ECR
9. **Deploy** (optional): Deploy to cloud

### Manual GitHub Actions Trigger
```bash
# The workflow runs automatically on:
git push origin main

# Or manually trigger from GitHub UI:
# Actions â†’ Select workflow â†’ Run workflow
```

### Setting Up CI/CD Secrets
In GitHub repository settings, add these secrets:
```
MONGODB_URI              â†’ Your MongoDB connection string
AWS_ACCESS_KEY_ID       â†’ Your AWS access key
AWS_SECRET_ACCESS_KEY   â†’ Your AWS secret key
DOCKER_USERNAME         â†’ Docker Hub username (optional)
DOCKER_PASSWORD         â†’ Docker Hub token (optional)
```

---


---

## ğŸ“Š Monitoring & Logging

### Log Files Location
```
logs/
â”œâ”€â”€ MM_DD_YYYY_HH_MM_SS.log
â””â”€â”€ (New log created for each run)
```

### View Real-time Logs
```bash
# Follow logs in real-time
tail -f logs/01_01_2025_10_30_45.log

# Search for errors
grep "ERROR" logs/*.log

# View specific component logs
grep "DataTransformation" logs/*.log
```

### MLflow UI
```bash
# Start MLflow UI
mlflow ui --host 0.0.0.0 --port 5000

# Access at: http://localhost:5000
```

---

## ğŸ“ˆ Performance Metrics

### Expected Model Performance
- **F1 Score**: 0.80-0.95 (depending on dataset)
- **Precision**: 0.80-0.90
- **Recall**: 0.75-0.90
- **Training Time**: 2-5 minutes (depends on hardware)

### Data Statistics
- **Total Records**: ~11,000 (phisingData.csv)
- **Features**: 30
- **Target Classes**: Binary (0, 1)
- **Missing Values**: Handled by KNN Imputation

---

---

## ğŸ“š Additional Resources

- **FastAPI Documentation**: https://fastapi.tiangolo.com/
- **MLflow Documentation**: https://mlflow.org/docs/
- **MongoDB Documentation**: https://docs.mongodb.com/
- **AWS S3 Documentation**: https://docs.aws.amazon.com/s3/
- **scikit-learn Documentation**: https://scikit-learn.org/

---

## ğŸ“„ License

This project is licensed under the GNU General Public License v3.0 - see LICENSE file for details.

---


## ğŸ¤ Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

